{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524eb44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import subprocess\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f718391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google API Key exists and begins AI\n",
      "Groq API Key exists and begins gsk_\n",
      "OpenRouter API Key exists and begins sk-or-\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "google_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")\n",
    "\n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:6]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f423782",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=groq_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cefd62e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"gemini\": \"gemini-2.5-pro\", \"groq\": \"openai/gpt-oss-120b\"}\n",
    "\n",
    "clients = {\"gemini-2.5-pro\": gemini, \"openai/gpt-oss-120b\": groq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bec10909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'platform': 'Linux-6.17.0-8-generic-x86_64-with-glibc2.42',\n",
       "  'python_version': '3.13.7',\n",
       "  'machine': 'x86_64',\n",
       "  'processor': ''},\n",
       " {'rustc': 'rustc not found', 'cargo': 'cargo not found'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "import shutil\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def retrieve_system_info():\n",
    "    return {\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python_version\": platform.python_version(),\n",
    "        \"machine\": platform.machine(),\n",
    "        \"processor\": platform.processor(),\n",
    "    }\n",
    "\n",
    "\n",
    "def rust_toolchain_info():\n",
    "    rustc_path = shutil.which(\"rustc\")\n",
    "    cargo_path = shutil.which(\"cargo\")\n",
    "    rustc_ver = (\n",
    "        subprocess.check_output([\"rustc\", \"--version\"], text=True).strip()\n",
    "        if rustc_path\n",
    "        else \"rustc not found\"\n",
    "    )\n",
    "    cargo_ver = (\n",
    "        subprocess.check_output([\"cargo\", \"--version\"], text=True).strip()\n",
    "        if cargo_path\n",
    "        else \"cargo not found\"\n",
    "    )\n",
    "    return {\n",
    "        \"rustc\": rustc_ver,\n",
    "        \"cargo\": cargo_ver,\n",
    "    }\n",
    "\n",
    "\n",
    "system_info = retrieve_system_info()\n",
    "rust_info = rust_toolchain_info()\n",
    "\n",
    "\n",
    "system_info, rust_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22ea2f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Short answer:** Yes ‚Äì you need to install a Rust tool‚Äëchain.  \n",
       "Below are the **minimal, one‚Äëline steps** to get a fully‚Äëfunctional `rustc` (and `cargo`) on your Linux machine, followed by the exact `compile_command` and `run_command` you can paste into Python.\n",
       "\n",
       "---\n",
       "\n",
       "## 1Ô∏è‚É£ Install the Rust tool‚Äëchain (rustup)\n",
       "\n",
       "```bash\n",
       "# 1Ô∏è‚É£ Download and run the official installer (non‚Äëinteractive)\n",
       "curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
       "\n",
       "# 2Ô∏è‚É£ Add Rust to the current shell (or restart the terminal later)\n",
       "source \"$HOME/.cargo/env\"\n",
       "```\n",
       "\n",
       "*What this does*  \n",
       "\n",
       "- Installs **rustup** (the official Rust version manager).  \n",
       "- Pulls the latest stable tool‚Äëchain (`rustc`, `cargo`, `rust-std`, ‚Ä¶).  \n",
       "- Puts `~/.cargo/bin` on your `PATH` so `rustc` and `cargo` are instantly usable.\n",
       "\n",
       "> **Verify** it worked:  \n",
       "> ```bash\n",
       "> rustc --version   # e.g. rustc 1.78.0 (9c6c5b7c1 2024-07-23)\n",
       "> cargo --version\n",
       "> ```\n",
       "\n",
       "---\n",
       "\n",
       "## 2Ô∏è‚É£ Compile for **maximum runtime performance**\n",
       "\n",
       "When you care more about the speed of the produced binary than about compile time, use the most aggressive optimisation flags:\n",
       "\n",
       "```bash\n",
       "rustc -C opt-level=3 \\\n",
       "      -C target-cpu=native \\\n",
       "      -C lto=yes \\\n",
       "      -C codegen-units=1 \\\n",
       "      -C panic=abort \\\n",
       "      -C strip=symbols \\\n",
       "      main.rs -o main\n",
       "```\n",
       "\n",
       "Explanation of the flags  \n",
       "\n",
       "| Flag | Why it helps runtime |\n",
       "|------|----------------------|\n",
       "| `-C opt-level=3` | Highest optimisation level. |\n",
       "| `-C target-cpu=native` | Generates code tuned for *your* CPU (e.g., AVX, SSE). |\n",
       "| `-C lto=yes` | Link‚Äëtime optimisation ‚Äì whole‚Äëprogram analysis. |\n",
       "| `-C codegen-units=1` | Forces a single code‚Äëgen unit ‚Üí better inlining & optimisation. |\n",
       "| `-C panic=abort` | Removes unwind code; panics abort immediately (smaller & faster). |\n",
       "| `-C strip=symbols` | Removes debug symbols, reducing binary size (no effect on speed). |\n",
       "\n",
       "---\n",
       "\n",
       "## 3Ô∏è‚É£ Run the binary\n",
       "\n",
       "```bash\n",
       "./main\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "## 4Ô∏è‚É£ Put it together in Python\n",
       "\n",
       "```python\n",
       "import subprocess\n",
       "from pathlib import Path\n",
       "\n",
       "# -------------------------------------------------\n",
       "# 1Ô∏è‚É£ Compile (fastest possible runtime performance)\n",
       "# -------------------------------------------------\n",
       "compile_command = [\n",
       "    \"rustc\",\n",
       "    \"-C\", \"opt-level=3\",\n",
       "    \"-C\", \"target-cpu=native\",\n",
       "    \"-C\", \"lto=yes\",\n",
       "    \"-C\", \"codegen-units=1\",\n",
       "    \"-C\", \"panic=abort\",\n",
       "    \"-C\", \"strip=symbols\",\n",
       "    \"main.rs\",          # source file\n",
       "    \"-o\", \"main\"        # output executable\n",
       "]\n",
       "\n",
       "compile_result = subprocess.run(\n",
       "    compile_command,\n",
       "    check=True,\n",
       "    text=True,\n",
       "    capture_output=True\n",
       ")\n",
       "print(\"Compilation stdout:\", compile_result.stdout)\n",
       "print(\"Compilation stderr:\", compile_result.stderr)\n",
       "\n",
       "# -------------------------------------------------\n",
       "# 2Ô∏è‚É£ Execute the compiled program\n",
       "# -------------------------------------------------\n",
       "run_command = [\"./main\"]          # the binary we just built\n",
       "run_result = subprocess.run(\n",
       "    run_command,\n",
       "    check=True,\n",
       "    text=True,\n",
       "    capture_output=True\n",
       ")\n",
       "\n",
       "print(\"Program output:\", run_result.stdout)\n",
       "```\n",
       "\n",
       "*What the Python code does*  \n",
       "\n",
       "1. **Compiles** `main.rs` with the aggressive optimisation flags above.  \n",
       "2. **Runs** the resulting executable (`./main`).  \n",
       "3. Captures and returns the program‚Äôs `stdout`.\n",
       "\n",
       "> **Tip:** If you ever need a quick ‚Äújust‚Äëworks‚Äù compile (e.g., for debugging), you can drop all `-C ‚Ä¶` flags and just run `rustc main.rs -O`. The flags shown are the *maximum‚Äëperformance* configuration.\n",
       "\n",
       "---\n",
       "\n",
       "## 5Ô∏è‚É£ (Optional) Verify the binary is truly native‚Äëoptimised\n",
       "\n",
       "```bash\n",
       "# Show the CPU‚Äëspecific instructions used (e.g., AVX2, SSE4.2, ‚Ä¶)\n",
       "objdump -d -M intel main | grep -E \"avx|sse\"\n",
       "```\n",
       "\n",
       "If you see AVX/SSE instructions, the `target-cpu=native` flag worked.\n",
       "\n",
       "---\n",
       "\n",
       "### üéâ You‚Äôre now ready\n",
       "\n",
       "* Rust is installed.  \n",
       "* You can compile a single `main.rs` file into the fastest possible executable for your machine.  \n",
       "* The exact `compile_command` and `run_command` for your Python workflow are provided above.\n",
       "\n",
       "Happy coding! üöÄ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "message = f\"\"\"\n",
    "Here is a report of the system information for my computer.\n",
    "I want to run a Rust compiler to compile a single rust file called main.rs and then execute it in the simplest way possible.\n",
    "Please reply with whether I need to install a Rust toolchain to do this. If so, please provide the simplest step by step instructions to do so.\n",
    "\n",
    "If I'm already set up to compile Rust code, then I'd like to run something like this in Python to compile and execute the code:\n",
    "```python\n",
    "compile_command = # something here - to achieve the fastest possible runtime performance\n",
    "compile_result = subprocess.run(compile_command, check=True, text=True, capture_output=True)\n",
    "run_command = # something here\n",
    "run_result = subprocess.run(run_command, check=True, text=True, capture_output=True)\n",
    "return run_result.stdout\n",
    "```\n",
    "Please tell me exactly what I should use for the compile_command and run_command.\n",
    "Have the maximum possible runtime performance in mind; compile time can be slow. Fastest possible runtime performance for this platform is key.\n",
    "Reply with the commands in markdown.\n",
    "\n",
    "System information:\n",
    "{system_info}\n",
    "\n",
    "Rust toolchain information:\n",
    "{rust_info}\n",
    "\"\"\"\n",
    "\n",
    "response = groq.chat.completions.create(\n",
    "    model=models[\"groq\"], messages=[{\"role\": \"user\", \"content\": message}]\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf8f0f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_command = [\n",
    "    \"/Users/ed/.cargo/bin/rustc\",\n",
    "    \"main.rs\",\n",
    "    \"-C\",\n",
    "    \"opt-level=3\",\n",
    "    \"-C\",\n",
    "    \"target-cpu=native\",\n",
    "    \"-C\",\n",
    "    \"codegen-units=1\",\n",
    "    \"-C\",\n",
    "    \"lto=fat\",\n",
    "    \"-C\",\n",
    "    \"panic=abort\",\n",
    "    \"-C\",\n",
    "    \"strip=symbols\",\n",
    "    \"-o\",\n",
    "    \"main\",\n",
    "]\n",
    "\n",
    "run_command = [\"./main\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "296a2a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"Rust\"  # or \"C++\"\n",
    "extension = \"rs\" if language == \"Rust\" else \"cpp\"\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "Your task is to convert Python code into high performance {language} code.\n",
    "Respond only with {language} code. Do not provide any explanation other than occasional comments.\n",
    "The {language} response needs to produce an identical output in the fastest possible time.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def user_prompt_for(python):\n",
    "    return f\"\"\"\n",
    "Port this Python code to {language} with the fastest possible implementation that produces identical output in the least time.\n",
    "The system information is:\n",
    "{system_info}\n",
    "Your response will be written to a file called main.{language} and then compiled and executed; the compilation command is:\n",
    "{compile_command}\n",
    "Respond only with {language} code.\n",
    "Python code to port:\n",
    "\n",
    "```python\n",
    "{python}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d16496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(python):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(python)},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d11b4f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(code):\n",
    "    with open(f\"main.{extension}\", \"w\") as f:\n",
    "        f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b361438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def port(model, python):\n",
    "    client = clients[model]\n",
    "    reasoning_effort = \"high\" if \"gpt\" in model else None\n",
    "    response = client.chat.completions.create(\n",
    "        model=model, messages=messages_for(python), reasoning_effort=reasoning_effort\n",
    "    )\n",
    "    reply = response.choices[0].message.content\n",
    "    reply = reply.replace(\"```cpp\", \"\").replace(\"```rust\", \"\").replace(\"```\", \"\")\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05fe0f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_python(code):\n",
    "    globals_dict = {\"__builtins__\": __builtins__}\n",
    "\n",
    "    buffer = io.StringIO()\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = buffer\n",
    "\n",
    "    try:\n",
    "        exec(code, globals_dict)\n",
    "        output = buffer.getvalue()\n",
    "    except Exception as e:\n",
    "        output = f\"Error: {e}\"\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dc3a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_hard = \"\"\"\n",
    "# Be careful to support large numbers\n",
    "\n",
    "def lcg(seed, a=1664525, c=1013904223, m=2**32):\n",
    "    value = seed\n",
    "    while True:\n",
    "        value = (a * value + c) % m\n",
    "        yield value\n",
    "\n",
    "\n",
    "def max_subarray_sum(n, seed, min_val, max_val):\n",
    "    lcg_gen = lcg(seed)\n",
    "\n",
    "    random_numbers = [\n",
    "        next(lcg_gen) % (max_val - min_val + 1) + min_val\n",
    "        for _ in range(n)\n",
    "    ]\n",
    "\n",
    "    max_sum = float(\"-inf\")\n",
    "\n",
    "    for i in range(n):\n",
    "        current_sum = 0\n",
    "        for j in range(i, n):\n",
    "            current_sum += random_numbers[j]\n",
    "            if current_sum > max_sum:\n",
    "                max_sum = current_sum\n",
    "\n",
    "    return max_sum\n",
    "\n",
    "\n",
    "def total_max_subarray_sum(n, initial_seed, min_val, max_val):\n",
    "    total_sum = 0\n",
    "    lcg_gen = lcg(initial_seed)\n",
    "\n",
    "    for _ in range(20):\n",
    "        seed = next(lcg_gen)\n",
    "        total_sum += max_subarray_sum(n, seed, min_val, max_val)\n",
    "\n",
    "    return total_sum\n",
    "\n",
    "\n",
    "# Parameters\n",
    "n = 10000          # Number of random numbers\n",
    "initial_seed = 42  # Initial seed for the LCG\n",
    "min_val = -10      # Minimum value of random numbers\n",
    "max_val = 10       # Maximum value of random numbers\n",
    "\n",
    "\n",
    "# Timing the function\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "result = total_max_subarray_sum(n, initial_seed, min_val, max_val)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total Maximum Subarray Sum (20 runs):\", result)\n",
    "print(\"Execution Time: {:.6f} seconds\".format(end_time - start_time))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d388a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_run(code):\n",
    "    write_output(code)\n",
    "    try:\n",
    "        subprocess.run(compile_command, check=True, text=True, capture_output=True)\n",
    "        run_result = subprocess.run(\n",
    "            run_command, check=True, text=True, capture_output=True\n",
    "        )\n",
    "        return run_result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"An error occurred:\\n{e.stderr}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "612bb2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSS = \"\"\"\n",
    ":root {\n",
    "  --py-color: #209dd7;\n",
    "  --cpp-color: #ecad0a;\n",
    "  --accent:   #753991;\n",
    "  --card:     #161a22;\n",
    "  --text:     #e9eef5;\n",
    "}\n",
    "\n",
    "/* Full-width layout */\n",
    ".gradio-container {\n",
    "  max-width: 100% !important;\n",
    "  padding: 0 40px !important;\n",
    "}\n",
    "\n",
    "/* Code card styling */\n",
    ".card {\n",
    "  background: var(--card);\n",
    "  border: 1px solid rgba(255,255,255,.08);\n",
    "  border-radius: 14px;\n",
    "  padding: 10px;\n",
    "}\n",
    "\n",
    "/* Buttons */\n",
    ".convert-btn button {\n",
    "  background: var(--accent) !important;\n",
    "  border-color: rgba(255,255,255,.12) !important;\n",
    "  color: white !important;\n",
    "  font-weight: 700;\n",
    "}\n",
    ".run-btn button {\n",
    "  background: #202631 !important;\n",
    "  color: var(--text) !important;\n",
    "  border-color: rgba(255,255,255,.12) !important;\n",
    "}\n",
    ".run-btn.py button:hover  { box-shadow: 0 0 0 2px var(--py-color) inset; }\n",
    ".run-btn.cpp button:hover { box-shadow: 0 0 0 2px var(--cpp-color) inset; }\n",
    ".convert-btn button:hover { box-shadow: 0 0 0 2px var(--accent) inset; }\n",
    "\n",
    "/* Outputs with color tint */\n",
    ".py-out textarea {\n",
    "  background: linear-gradient(180deg, rgba(32,157,215,.18), rgba(32,157,215,.10));\n",
    "  border: 1px solid rgba(32,157,215,.35) !important;\n",
    "  color: rgba(32,157,215,1) !important;\n",
    "  font-weight: 600;\n",
    "}\n",
    ".cpp-out textarea {\n",
    "  background: linear-gradient(180deg, rgba(236,173,10,.22), rgba(236,173,10,.12));\n",
    "  border: 1px solid rgba(236,173,10,.45) !important;\n",
    "  color: rgba(236,173,10,1) !important;\n",
    "  font-weight: 600;\n",
    "}\n",
    "\n",
    "/* Align controls neatly */\n",
    ".controls .wrap {\n",
    "  gap: 10px;\n",
    "  justify-content: center;\n",
    "  align-items: center;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ae85ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2] Sandbox: CanCreateUserNamespace() clone() failure: EPERM\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks(\n",
    "    css=CSS, theme=gr.themes.Monochrome(), title=f\"Port from Python to {language}\"\n",
    ") as ui:\n",
    "    with gr.Row(equal_height=True):\n",
    "        with gr.Column(scale=6):\n",
    "            python = gr.Code(\n",
    "                label=\"Python (original)\",\n",
    "                value=python_hard,\n",
    "                language=\"python\",\n",
    "                lines=26,\n",
    "            )\n",
    "        with gr.Column(scale=6):\n",
    "            cpp = gr.Code(\n",
    "                label=f\"{language} (generated)\", value=\"\", language=\"cpp\", lines=26\n",
    "            )\n",
    "\n",
    "    with gr.Row(elem_classes=[\"controls\"]):\n",
    "        python_run = gr.Button(\"Run Python\", elem_classes=[\"run-btn\", \"py\"])\n",
    "        model_options = list(clients.keys())\n",
    "        model = gr.Dropdown(\n",
    "            choices=model_options, value=model_options[0], show_label=False\n",
    "        )\n",
    "        convert = gr.Button(f\"Port to {language}\", elem_classes=[\"convert-btn\"])\n",
    "        cpp_run = gr.Button(f\"Run {language}\", elem_classes=[\"run-btn\", \"cpp\"])\n",
    "\n",
    "    with gr.Row(equal_height=True):\n",
    "        with gr.Column(scale=6):\n",
    "            python_out = gr.TextArea(\n",
    "                label=\"Python result\", lines=8, elem_classes=[\"py-out\"]\n",
    "            )\n",
    "        with gr.Column(scale=6):\n",
    "            cpp_out = gr.TextArea(\n",
    "                label=f\"{language} result\", lines=8, elem_classes=[\"cpp-out\"]\n",
    "            )\n",
    "\n",
    "    convert.click(fn=port, inputs=[model, python], outputs=[cpp])\n",
    "    python_run.click(fn=run_python, inputs=[python], outputs=[python_out])\n",
    "    cpp_run.click(fn=compile_and_run, inputs=[cpp], outputs=[cpp_out])\n",
    "\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
